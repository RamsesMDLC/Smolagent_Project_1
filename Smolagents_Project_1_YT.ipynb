{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs1PHDw+3vybxnLVzCMKQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamsesMDLC/Smolagent_Project_1/blob/main/Smolagents_Project_1_YT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. LOADING LIBRARIES / MODULES / CLASSES**"
      ],
      "metadata": {
        "id": "3FV-Eujbs2v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installs the smolagents library along with extensions defined in the [toolkit] option.\n",
        "!pip install smolagents[toolkit]\n",
        "\n",
        "#Components from smolagents\n",
        "  #CodeAgent: The \"agent\". It orchestrates reasoning and tool usage.\n",
        "  #DuckDuckGoSearchTool: The \"tool\". It lets the agent fetch information from the web.\n",
        "  #TransformersModel: It \"allow us to get access to the model through Hugging Face\". A wrapper for Hugging Face Transformer models.\n",
        "from smolagents import CodeAgent, DuckDuckGoSearchTool, TransformersModel\n",
        "\n",
        "#API key\n",
        "  #Provides a secure way to access stored secrets (like API tokens) within Google Colab.\n",
        "from google.colab import userdata\n",
        "  #Allows programmatic login to Hugging Face Hub.\n",
        "from huggingface_hub import login\n",
        "\n",
        "#Tokenizer: class in the Hugging Face Transformers library to process text inputs (\"prompts or text\") and outputs (\"answer\") for the model.\n",
        "  #This means AutoTokenizer forms the bridge:\n",
        "    #Input text → tokens/tensors → Model\n",
        "      #Splitting text into tokens (smaller pieces such as words or subwords).\n",
        "      #Converting these tokens into numbers (\"tensors\"), called input IDs, which the model uses for computation.\n",
        "      #Managing extra elements like special tokens (e.g., [CLS], [SEP], padding).\n",
        "    #Model output tokens/tensors → decoded text\n",
        "  #It automatically loads and configures the correct tokenizer for a specified model (i.e., there’s no need to know the model-specific tokenizer class).\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0478b3-048d-4880-998a-c8a28384a816",
        "id": "cqdC1mG4su_5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting smolagents[toolkit]\n",
            "  Downloading smolagents-1.21.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.31.2 in /usr/local/lib/python3.12/dist-packages (from smolagents[toolkit]) (0.34.4)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from smolagents[toolkit]) (2.32.4)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from smolagents[toolkit]) (13.9.4)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from smolagents[toolkit]) (3.1.6)\n",
            "Requirement already satisfied: pillow>=10.0.1 in /usr/local/lib/python3.12/dist-packages (from smolagents[toolkit]) (11.3.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from smolagents[toolkit]) (1.1.1)\n",
            "Collecting ddgs>=9.0.0 (from smolagents[toolkit])\n",
            "  Downloading ddgs-9.5.5-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting markdownify>=0.14.1 (from smolagents[toolkit])\n",
            "  Downloading markdownify-1.2.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs>=9.0.0->smolagents[toolkit]) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from ddgs>=9.0.0->smolagents[toolkit])\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting lxml>=6.0.0 (from ddgs>=9.0.0->smolagents[toolkit])\n",
            "  Downloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents[toolkit]) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents[toolkit]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents[toolkit]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents[toolkit]) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents[toolkit]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents[toolkit]) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.31.2->smolagents[toolkit]) (1.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.4->smolagents[toolkit]) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.12/dist-packages (from markdownify>=0.14.1->smolagents[toolkit]) (4.13.5)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.12/dist-packages (from markdownify>=0.14.1->smolagents[toolkit]) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents[toolkit]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents[toolkit]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents[toolkit]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->smolagents[toolkit]) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->smolagents[toolkit]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->smolagents[toolkit]) (2.19.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.9->markdownify>=0.14.1->smolagents[toolkit]) (2.8)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents[toolkit]) (0.1.2)\n",
            "Downloading ddgs-9.5.5-py3-none-any.whl (37 kB)\n",
            "Downloading markdownify-1.2.0-py3-none-any.whl (15 kB)\n",
            "Downloading smolagents-1.21.3-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, lxml, markdownify, ddgs, smolagents\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "Successfully installed ddgs-9.5.5 lxml-6.0.1 markdownify-1.2.0 primp-0.15.0 smolagents-1.21.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Securely get Hugging Face token and login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(hf_token)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "else:\n",
        "    print(\"Token not found. Please add HF_TOKEN secret.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_F6zBFjs0BT",
        "outputId": "95dcae5c-ab26-4b7f-b328-9116aa8b216b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    if tokenizer.pad_token is None:\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "   \n",
        "Why this matters\n",
        "\n",
        "Padding tokens are critical for batch processing inputs of varying lengths.\n",
        "\n",
        "A padding token is a special token used in natural language processing (NLP) to make input sequences (like sentences or documents) have the same fixed length when processed by machine learning models.\n",
        "Why is it needed?\n",
        "\n",
        "    Most NLP models, especially deep learning models like transformers or RNNs, require inputs to be uniform in length.\n",
        "\n",
        "    Real-world texts vary in length, so to batch-process multiple sequences efficiently, shorter sequences are padded with these special tokens until they match the longest sequence length in the batch.\n",
        "\n",
        "    Padding tokens carry no meaningful information and are meant only to fill space for model input consistency.\n",
        "\n",
        "Example:\n",
        "\n",
        "If you have these sentences tokenized into token IDs:\n",
        "\n",
        "    Sentence 1: [3456] (length 6)\n",
        "\n",
        "    Sentence 2: (length 2)\n",
        "\n",
        "To process them together, Sentence 2 might be padded with four pad tokens (often token ID 0):\n",
        "\n",
        "    Sentence 2 padded:\n",
        "\n",
        "How it works in the model:\n",
        "\n",
        "    The model uses attention masks (binary flags) to ignore these padding tokens during processing, so they do not affect predictions or training loss.\n",
        "\n",
        "    Padding preserves the position of real tokens in the sequence, allowing consistent indexing.\n",
        "\n",
        "Summary\n",
        "\n",
        "Padding tokens enable models to handle variable-length text inputs by standardizing them into fixed-size sequences, allowing efficient batch training and inference while maintaining the order and meaning of the original text content.\n",
        "\n",
        "This makes it possible for NLP models to process multiple texts simultaneously without errors or inefficiencies caused by varying input lengths."
      ],
      "metadata": {
        "id": "-F2TCkYo8jfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Model (from Hugging Face)\n",
        "model_id = \"Qwen/Qwen1.5-1.8B\"\n",
        "\n",
        "#Initialize tokenizer\n",
        "  #Load a pretrained tokenizer for the given model identified by model_id (in this case \"Qwen/Qwen1.5-1.8B\")\n",
        "    #The tokenizer includes vocabulary, tokenization rules, special tokens, and associated settings needed to convert raw text into token IDs.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "#Check whether the tokenizer has a designated padding token.\n",
        "  #Padding tokens are used to make all input sequences the same length by adding special \"pad\" tokens to shorter sequences.\n",
        "\n",
        "#If the padding token is not set, the tokenizer or model might throw errors during inference or training.\n",
        "\n",
        "#By assigning the EOS token as padding, the code ensures compatibility even when a dedicated pad token is not defined for the particular model.\n",
        "# Summary\n",
        "\n",
        "# The code safely loads the tokenizer for a model and guarantees it has a valid padding token by assigning it to the EOS token if missing. This ensures stable input preprocessing and model compatibility during tokenization and generation\n",
        "\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model\n",
        "model = TransformersModel(model_id=model_id)\n",
        "\n",
        "# Fix pad_token_id in model config if not set\n",
        "if model.model.config.pad_token_id is None:\n",
        "    model.model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "t3D7sQIS6eOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize agent\n",
        "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\n",
        "\n",
        "# Prepare input text and tokenize with attention mask\n",
        "input_text = \"How long would it take for an elephant to cross the United States from Florida to California?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "# Since agent.run() might not allow passing attention_mask directly,\n",
        "# call the model generation yourself for reliable behavior:\n",
        "\n",
        "generated_ids = model.model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    pad_token_id=model.model.config.pad_token_id,\n",
        "    max_new_tokens=50\n",
        ")\n",
        "\n",
        "# Decode generated tokens\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "mha4bwi25NCv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}